{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_team_notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cenv8tx2CQMW",
        "colab_type": "text"
      },
      "source": [
        "# Team's notebook for final model\n",
        "\n",
        "This was our team's final notebook used to generate our model used in the submission.  \n",
        "\n",
        "**Note: Set your google colab runtime type to GPU(You can thank me later :p)**  \n",
        "\n",
        "If you are looking for the benchmark models notebook or the object detection notebook, please look for those notebooks respectively.\n",
        "\n",
        "Note: I have covered most of what is happening here in the other notebook, hence why the short descriptions. If you want a more detailed walkthrough, I suggest you look through the benchmark models notebook.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBwbuj7yC_4R",
        "colab_type": "text"
      },
      "source": [
        "# Setting up the notebook\n",
        "\n",
        "Same as the other notebooks, we will setting up the main directories and importing the libraries and modules used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6xIs7vUGjxu",
        "colab_type": "text"
      },
      "source": [
        "In order to mount our google drive folder to our colab notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wte7ugBLCKkJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "81a0a1f8-1b46-4951-be0f-9b45bf7fcbae"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqOP7lq0Govc",
        "colab_type": "text"
      },
      "source": [
        "Importing main libraries and modules\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9OUL-cWGFzS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96f49f05-4d47-4609-91c0-3500be370e56"
      },
      "source": [
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, BatchNormalization, Activation, GlobalAveragePooling2D, Dense, Dropout, MaxPooling2D\n",
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, Model \n",
        "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from keras import backend as k \n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
        "from keras.applications.vgg16 import VGG16 "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAKkpTEzGriN",
        "colab_type": "text"
      },
      "source": [
        "Setting up base directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gwfUVuSGIaF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "outputId": "aa599808-bbde-4aee-8c39-2f43bae2cd4e"
      },
      "source": [
        "import os\n",
        "\n",
        "base_dir = 'gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset'\n",
        "\n",
        "# Directory to our training data\n",
        "train_folder = os.path.join(base_dir, 'train')\n",
        "\n",
        "# Directory to our validation data\n",
        "val_folder = os.path.join(base_dir, 'val')\n",
        "\n",
        "\n",
        "# List folders and number of files\n",
        "print(\"Directory, Number of files\")\n",
        "for root, subdirs, files in os.walk(base_dir):\n",
        "    print(root, len(files))\n",
        "    \n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Batch size\n",
        "bs = 16\n",
        "\n",
        "# All images will be resized to this value\n",
        "image_size = (224, 224)\n",
        "\n",
        "# All images will be rescaled by 1./255. We apply data augmentation here.\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   brightness_range= [0.5,1.5],\n",
        "                                   horizontal_flip=True,\n",
        "                                  rotation_range=40,\n",
        "                                  width_shift_range=(-100,100),\n",
        "                                  height_shift_range=(-100,100))\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 32 using train_datagen generator\n",
        "print(\"Preparing generator for train dataset\")\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory= train_folder, # This is the source directory for training images \n",
        "    target_size=image_size, # All images will be resized to value set in image_size\n",
        "    batch_size=bs,\n",
        "    class_mode='categorical')\n",
        "\n",
        "# Flow validation images in batches of 32 using val_datagen generator\n",
        "print(\"Preparing generator for validation dataset\")\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    directory= val_folder, \n",
        "    target_size=image_size,\n",
        "    batch_size=bs,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directory, Number of files\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset 0\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val 0\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val/EaglePose 27\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val/ChestBump 17\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val/ChildPose 22\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val/Dabbing 23\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val/ChairPose 23\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val/Spiderman 26\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val/HandGun 26\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val/WarriorPose 22\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val/KungfuSalute 25\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val/Salute 24\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val/KoreanHeart 22\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val/KungfuCrane 21\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val/HulkSmash 24\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val/HighKneel 24\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/val/HandShake 21\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train 0\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train/HandGun 94\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train/ChildPose 81\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train/HandShake 83\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train/HulkSmash 88\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train/HighKneel 96\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train/KoreanHeart 87\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train/Dabbing 89\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train/ChestBump 56\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train/ChairPose 89\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train/EaglePose 96\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train/WarriorPose 93\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train/Salute 90\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train/Spiderman 101\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train/KungfuSalute 92\n",
            "gdrive/My Drive/Brainhack_TIL_2019_Final/Dataset/train/KungfuCrane 88\n",
            "Preparing generator for train dataset\n",
            "Found 1322 images belonging to 15 classes.\n",
            "Preparing generator for validation dataset\n",
            "Found 347 images belonging to 15 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4PL6PzdHh0L",
        "colab_type": "text"
      },
      "source": [
        "# Constructing the model's architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rDMENp0GL3g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "35776d0b-f144-4eed-bfb5-c29733dd0c05"
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "import keras.backend as K\n",
        "from keras.applications.resnet50 import ResNet50 \n",
        "from keras.layers.core import Flatten, Dense, Dropout\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras import optimizers\n",
        "import cv2, numpy as np\n",
        "\n",
        "# Here we specify the input shape of our data \n",
        "# This should match the size of images ('image_size') along with the number of channels (3)\n",
        "input_shape = (224, 224,3)\n",
        "input_img = Input(shape=input_shape)\n",
        "# Define the number of classes\n",
        "num_classes = 11\n",
        "\n",
        "# Defining a baseline model. Here we use the [keras functional api](https://keras.io/getting-started/functional-api-guide) to build the model. \n",
        "# TODO: explore different architectures and training schemes\n",
        "x=ZeroPadding2D((3,3))(input_img)\n",
        "x=keras.applications.vgg16.VGG16(include_top=False, classes=15, input_shape = (224, 224,3), pooling='avg')(x)\n",
        "\n",
        "\n",
        "x=Dropout(0.5)(x)\n",
        "x=Dense(128, activation='relu')(x)\n",
        "x=Dropout(0.5)(x)\n",
        "predictions = Dense(15, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=input_img, outputs=predictions)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0621 03:38:11.447985 139723782072192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0621 03:38:11.495411 139723782072192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0621 03:38:11.545013 139723782072192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0621 03:38:11.595243 139723782072192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0621 03:38:14.310295 139723782072192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0621 03:38:14.311811 139723782072192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0621 03:38:14.935265 139723782072192 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW23OuznIO_U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "56d84208-bc7f-449b-c3d1-0905f6d9e608"
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.SGD(nesterov=True),\n",
        "              metrics=['accuracy'])\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "bestValidationCheckpointer = ModelCheckpoint('saved_model.hdf5', monitor='val_acc', save_best_only=True, verbose=1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0621 03:38:16.167474 139723782072192 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m99P8B5IP-_",
        "colab_type": "text"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeaW-bUCJJNx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "c0b64eae-eef3-48d2-8b61-b8d17a183801"
      },
      "source": [
        "history = model.fit_generator(\n",
        "        train_generator, # train generator has 973 train images\n",
        "        steps_per_epoch=train_generator.samples // bs + 1,\n",
        "        epochs=50,\n",
        "        validation_data=val_generator, # validation generator has 253 validation images\n",
        "        validation_steps=val_generator.samples // bs + 1,\n",
        "        callbacks=[bestValidationCheckpointer]\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0621 03:38:18.347114 139723782072192 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "18/83 [=====>........................] - ETA: 34:38 - loss: nan - acc: 0.0868"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e6f451d1fe38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# validation generator has 253 validation images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbestValidationCheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N50LYpcJIWPi",
        "colab_type": "text"
      },
      "source": [
        "# Submitting the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K0AvSs4ISvK",
        "colab_type": "text"
      },
      "source": [
        "Loading the saved model into the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0qk5QJUJO7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model_path = 'gdrive/My Drive/DSTA/bestsofar.hdf5'\n",
        "model = load_model( model_path )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOV8mcaMJS0Z",
        "colab_type": "text"
      },
      "source": [
        "From here on, the code was provided by DSTA to submit our model to their server's, as such I cannot comment much on the code except that it works and do not touch it unless you know what you are doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RDAFke3JEvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --force-reinstall --user --no-warn-script-location -q https://ai-camp.s3-us-west-2.amazonaws.com/AiCampEval-1.7-py3-none-any.whl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4lDIZDiJG5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from AiCampEval import eval_submit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTODfqHmJK1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "base_dir = 'gdrive/My Drive/DSTA'\n",
        "train_folder = os.path.join(base_dir, 'train')\n",
        "\n",
        "# Batch size\n",
        "bs = 32\n",
        "\n",
        "# All images will be resized to this value\n",
        "image_size = (224, 224)\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   brightness_range= [0.5,1.5],\n",
        "                                   horizontal_flip=True)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory= train_folder, # This is the source directory for training images \n",
        "    target_size=image_size, # All images will be resized to value set in image_size\n",
        "    batch_size=bs,\n",
        "    class_mode='categorical')\n",
        "\n",
        "print(train_generator.class_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L25WUAvJN7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "'''\n",
        "This is a helper function to preprocess the input before sending it through my network. \n",
        "It performs the following steps:\n",
        "1. Resizes each image to (224,224,3), which are the image dimensions my model was trained on.\n",
        "2. Normalizes the range of each pixel's value from [0-255] to [0-1].\n",
        "3. Reshapes each array such that it has a batch dimension: from  (224,224,3) -> (1,224,224,3), \n",
        "because my model.predict function expects a batch dimension.\n",
        "\n",
        "Implement your own preprocessing function according to your model's needs.\n",
        "''' \n",
        "def preprocess_arrays(list_of_np_arrays):\n",
        "    target_size = image_size\n",
        "    pil_images = [ Image.fromarray(arr.astype('uint8'), 'RGB') for arr in list_of_np_arrays ]\n",
        "    resized_pil_images = [img.resize( target_size, Image.NEAREST ) for img in pil_images]\n",
        "    resized_images = [np.array( img ) for img in resized_pil_images]\n",
        "    preprocessed_images = [np.expand_dims(x, axis=0) / 255. for x in resized_images]\n",
        "    return preprocessed_images\n",
        "\n",
        "'''\n",
        "This function accepts a list of numpy array images as input and outputs a list of prediction strings \n",
        "corresponding to each image in the input list.\n",
        "IMPORTANT: you have to implement this function.\n",
        "''' \n",
        "def evaluate_images(list_of_np_arrays):\n",
        "    # Swap the key and value in the class_indices\n",
        "    label_dict = dict((v,k) for k,v in (train_generator.class_indices).items())\n",
        "    # Run through the list_of_np_arrays to perform any preprocessing you need\n",
        "    preprocessed_imgs = preprocess_arrays( list_of_np_arrays )\n",
        "    # Get a list of softmax_vectors by passing in the preprocessed_imgs to model.predict()\n",
        "    softmax_vectors = [ model.predict( x )[0] for x in preprocessed_imgs ]\n",
        "    # Get the predicted_class_indices by running each of the softmax_vectors through np.argmax()\n",
        "    predicted_class_indices = [ np.argmax( x ) for x in softmax_vectors ]\n",
        "    # Convert the list of class_indices to a list of predictions (in str)\n",
        "    # predictions is a list of labels: ['KoreanHeart', 'KoreanHeart','ChairPose',.......]\n",
        "    predictions = [ label_dict[k] for k in predicted_class_indices ]\n",
        "    return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws5m3haDJQb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## CHANGE HERE\n",
        "TEAM_ID = \".NEET\"\n",
        "SUBMISSION_TYPE = \"thefinaltest_4_88888\"\n",
        "\n",
        "eval_submit(evaluate_images, SUBMISSION_TYPE, TEAM_ID )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}